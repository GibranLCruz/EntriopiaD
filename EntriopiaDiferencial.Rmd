---
title: "Aplicaciones de los momentos de una distribución"
subtitle: "La entropia diferencial"
author: "Gibran Aaron Loeza De la cruz"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: tango
    cards: false
---

```{r setup, include=FALSE}
#library(highcharter)
# invalidate cache when the tufte version changes
```




# Introducción

Los momentos de una distribución de probabilidad, la cual recordemos puede ser de dos tipos: con respecto al origen y centrales, se representan como:
$$
\mathbb{E}(X^n) = \int_{-\infty}^{\infty}{x^n f(x)dx}\\
\mathbb{E}([X-\mu]^n)= \int_{-\infty}^{\infty}{(x-\mu)^n f(x)dx}
$$
De igual forma, los momentos se pueden calcular para una función de una variable aleatoria y pueden tomar la siguiente forma:
$$
\mathbb{E}(g(X))= \int_{-\infty}^{\infty}{g(x)f(x)dx},
$$
donde $f(x)$ es la densidad de probabilidad y $g(x)$ es la la función sobre la variable aleatoria $X$. 
Una función de importancia práctica es la llamada entropía diferencial, la cual no es más que el momento aplicado a la función $g(x)=\log(f(x))$. Es decir, la entropía diferencial halla el logaritmo a la función de densidad.La entropía diferencial puede aplicarse a variables aleatorias continuas o discretas, ahora, sólo nos concentraremos en el cálculo de la entropía diferencia $h(X)$ de algunas densidades continuas conocidas. La entropía diferencial de una variable aleatoria $X$ con función de densidad $f(x)$ está entonces dada por la siguiente ecuación:
$$
h(X) = -\int_{-\infty}^{+\infty}{f(x) \log(f(x)) \, dx}
$$

Y por lo tanto si queremos hallar la entropía diferencial para la variable aleatoria uniforme $\mathcal{U}(a,b)$  que está definida como:

$$\mathcal{U}(a,b) = \begin{cases}
\frac{1}{b-a} & a < x < b\\
0 & \mbox{en otro caso}
\end{cases}
$$

entonces, el cálculo de su entropía diferencial debe ser efectuado de la siguiente forma; $h(\mathcal{U}(a,b)) = - \int_a^b{\frac{1}{b-a}\log(\frac{1}{b-a}), \, dx}$ y por lo tando, desarrollando esta ecuación, su entropía diferencial está dada por:

$$
\begin{align}
h(X) = & -\int_{a}^b{\left(\frac{1}{b-a}\right) \log(\frac{1}{b-a}) \, dx}\\
 = & -\left(\frac{1}{b-a}\right) \log(\frac{1}{b-a}) \times \int_a^b{dx}\\
 = & - \left(\frac{1}{b-a}\right) \log(\frac{1}{b-a})\times (b-a)\\
= & - \log(\frac{1}{b-a}) = \log(b-a)
\end{align}
$$

Por lo tanto $h(\mathcal{U}(a,b)) = \log(b-a)$. A continuación, como actividad se hallará la entropía diferencial para diferentes variables aleatorias continuas conocidas.


## Ejercicios.

Una vez visto la forma de calcular la entropía para densidades continuas, el turno será para calcular la entropía diferencial de las siguientes densidades:


1. $$f(x) = \begin{cases} \lambda \mbox{e}^{-\lambda x} & x \ge 0\\ 
0 & \mbox{en otro caso}\end{cases}$$

```{r}
pdf_expon <- function(x, lambda) {
  ifelse(x >= 0, lambda * exp(-lambda * x), 0)
}

entropy_expon <- function(lambda) {
  integrate(function(x) {
    pdf_val = pdf_expon(x, lambda)
    ifelse(pdf_val > 0, pdf_val * log(pdf_val), 0)
  }, lower = 0, upper = Inf)$value
}

lambda_valor <- 0.5
entropia_resultado <- entropy_expon(lambda_valor)
cat("Entropía diferencial para lambda =", lambda_valor, "es:", entropia_resultado, "\n")
```


2. $$f(x) = \begin{cases}\frac{1}{2}\lambda \mbox{e}^{-\lambda |x|} & -\infty < x < \infty\\
\end{cases}$$

```{r}
f <- function(x, lambda) {
  0.5 * lambda * exp(-lambda * abs(x))
}

entropy <- function(lambda) {
  integrand <- function(x) {
    fx <- f(x, lambda)
    return(ifelse(fx == 0, 0, fx * (log2(fx) - lambda * abs(x))))
  }
  resultado <- integrate(integrand, lower = 0, upper = Inf)$value
  return(resultado)
}

lambda_value <- 1
entropy_value <- entropy(lambda_value)
cat("La entropía diferencial es:", entropy_value, "\n")

```



3. $$f(x) = \frac{1}{\sigma \sqrt{2 \pi}}\mbox{e}^{-(x-\mu)^2/2\sigma^2}, -\infty < x < \infty, \, \sigma >0$$

```{r}
entropy_normal <- function(mu, sigma) {
  entropy <- 0.5 * (log(2 * pi * exp(1) * sigma^2) + 1)
  return(entropy)
}
mu <- 0
sigma <- 1


resultado <- entropy_normal(mu, sigma)
cat("La entropía diferencial es:", resultado, "\n")
```

4. $$f(x) = \begin{cases}\frac{1}{A\sqrt{A^2-x^2}} & -A < x < A\\
0 & \mbox{en otro caso}\end{cases}, \; \; A>0$$

```{r}

f <- function(x, A) {
  ifelse(x > -A & x < A, 1 / (A * sqrt(A^2 - x^2)), 0)
}
A <- integrate(function(x) 1 / sqrt(1 - x^2), -1, 1)$value
entropy <- function(x, A) {
  ifelse(x > 0 & x < A, -2 * 1 / (A * sqrt(A^2 - x^2)) * log(1 / (A * sqrt(A^2 - x^2))), 0)
}
resultado <- integrate(function(x) entropy(x, A), 0, A)$value
print(paste("La entropía diferencial es:", resultado))

```


Nota: pueden resolver la actividad en su cuaderno y posteriormente insertar en el HTML la imagen de sus resultados.

Atte. Dr.Julio César Ramírez Pacheco
